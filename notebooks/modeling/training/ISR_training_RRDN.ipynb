{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Adapted by Elena Gronskaya from ISR_Training_Tutorial.ipynb: https://github.com/idealo/image-super-resolution/tree/master/notebooks"
      ],
      "metadata": {
        "id": "jQtTmF9mnr-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook is to set up a model from the ISR repo and train it."
      ],
      "metadata": {
        "id": "K2cbvg2Enr5E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NANJnBwd6RNX"
      },
      "source": [
        "# Install ISR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEFfJc3S2Hqf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/image-super-resolution\n",
        "!python setup.py install\n",
        "\n",
        "# if using local repo of ISR\n",
        "# else use !pip install ISR and see ISR_module_adjustments notebook for changes\n",
        "# to run locally\n",
        "#!pip install gast>=0.3.2\n",
        "#!pip install ISR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'h5py==2.10.0' --force-reinstall\n",
        "!pip install -U PyYAML\n",
        "!pip install imagecodecs"
      ],
      "metadata": {
        "id": "tTirYgOjlOEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix-SoK9O6XlK"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQqwueZL6l5C"
      },
      "source": [
        "## Create the models\n",
        "Import the models from the ISR package and create\n",
        "\n",
        "- a RRDN super scaling network\n",
        "- a discriminator network for GANs training\n",
        "- a VGG19 feature extractor to train with a perceptual loss function\n",
        "\n",
        "Carefully select\n",
        "- 'x': this is the upscaling factor (2 by default)\n",
        "- 'layers_to_extract': these are the layers from the VGG19 that will be used in the perceptual loss (leave the default if you're not familiar with it)\n",
        "- 'lr_patch_size': this is the size of the patches that will be extracted from the LR images and fed to the ISR network during training time\n",
        "\n",
        "Play around with the other architecture parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKR5vgcO4wKN"
      },
      "outputs": [],
      "source": [
        "from ISR.models import RRDN\n",
        "from ISR.models import Discriminator\n",
        "from ISR.models import Cut_VGG19"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_train_patch_size = 60\n",
        "layers_to_extract = [5, 9]\n",
        "scale = 3\n",
        "hr_train_patch_size = lr_train_patch_size * scale\n",
        "\n",
        "rrdn  = RRDN(arch_params={'C':4, 'D':3, 'G':64, 'G0':64, 'T':10, 'x':scale}, patch_size=lr_train_patch_size)\n",
        "f_ext = Cut_VGG19(patch_size=hr_train_patch_size, layers_to_extract=layers_to_extract)\n",
        "discr = Discriminator(patch_size=hr_train_patch_size, kernel_size=3)"
      ],
      "metadata": {
        "id": "dAKRTZxgvSlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Give the models to the Trainer\n",
        "The Trainer object will combine the networks, manage your training data and keep you up-to-date with the training progress through Tensorboard and the command line.\n",
        "\n",
        "Here we do not use  the pixel-wise MSE but only the perceptual loss by specifying the respective weights in `loss_weights`"
      ],
      "metadata": {
        "id": "FbI-t2bOFeZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ISR.train import Trainer\n",
        "loss_weights = {\n",
        "  'generator': 1,\n",
        "  'feature_extractor' : 0,\n",
        "  'discriminator': 0\n",
        "}\n",
        "losses = {\n",
        "  'generator': 'mae',\n",
        "  'feature_extractor': 'mse',\n",
        "  'discriminator': 'binary_crossentropy'\n",
        "} \n",
        "\n",
        "log_dirs = {'logs': './logs', 'weights': './weights'}\n",
        "\n",
        "learning_rate = {'initial_value': 0.0004, 'decay_factor': .5, 'decay_frequency': 30}\n",
        "\n",
        "flatness = {'min': 0.0, 'max': 0.15, 'increase': 0.01, 'increase_frequency': 5}\n",
        "\n",
        "trainer = Trainer(\n",
        "    generator=rrdn,\n",
        "    discriminator=discr,\n",
        "    feature_extractor=f_ext,\n",
        "    lr_train_dir='PATH TO LR TRAINING DATA',\n",
        "    hr_train_dir='PATH TO HR TRAINING DATA',\n",
        "    lr_valid_dir='PATH TO LR VALIDATION DATA',\n",
        "    hr_valid_dir='PATH TO HR VALIDATION DATA',\n",
        "    loss_weights=loss_weights,\n",
        "    learning_rate=learning_rate,\n",
        "    flatness=flatness,\n",
        "    dataname='div2k',\n",
        "    log_dirs=log_dirs,\n",
        "    weights_generator=None,\n",
        "    weights_discriminator=None,\n",
        "    n_validation = 60\n",
        "    )"
      ],
      "metadata": {
        "id": "l9Kpl-RqFftj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose epoch number, steps and batch size and start training"
      ],
      "metadata": {
        "id": "35WBje7mFoxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cd to directory where you want the weights and TensorBoard logs to be saved\n",
        "cd /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "WSzUISRFFfwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(\n",
        "    epochs=400,\n",
        "    steps_per_epoch=40,\n",
        "    batch_size=4,\n",
        "    monitored_metrics={'val_generator_PSNR_Y':'max'})"
      ],
      "metadata": {
        "id": "EIpjMZLMFf0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train in a loop to test different hyperparameters\n"
      ],
      "metadata": {
        "id": "-fefq3H3979s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patch_sizes = [20, 40, 60, 80]\n",
        "batch_sizes = [2, 4, 8]\n",
        "learning_rates = [{'initial_value': 0.0004, 'decay_factor': .5, 'decay_frequency': 30},\n",
        "                  {'initial_value': 0.00004, 'decay_factor': .5, 'decay_frequency': 60},\n",
        "                  {'initial_value': 0.000004, 'decay_factor': .5, 'decay_frequency': 90}]"
      ],
      "metadata": {
        "id": "J9BfFc2b95b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "for patch in patch_sizes:\n",
        "  for batch in batch_sizes:\n",
        "    for lr in learning_rates:\n",
        "      try:\n",
        "        lr_train_patch_size = patch\n",
        "        layers_to_extract = [5, 9]\n",
        "        scale = 3\n",
        "        hr_train_patch_size = lr_train_patch_size * scale\n",
        "\n",
        "        rrdn  = RRDN(arch_params={'C':4, 'D':3, 'G':64, 'G0':64, 'T':10, 'x':scale}, patch_size=lr_train_patch_size)\n",
        "        f_ext = Cut_VGG19(patch_size=hr_train_patch_size, layers_to_extract=layers_to_extract)\n",
        "        discr = Discriminator(patch_size=hr_train_patch_size, kernel_size=3)\n",
        "\n",
        "        from ISR.train import Trainer\n",
        "        loss_weights = {\n",
        "          'generator': 1,\n",
        "          'feature_extractor' : 0,\n",
        "          'discriminator': 0\n",
        "        }\n",
        "        losses = {\n",
        "          'generator': 'mae',\n",
        "          'feature_extractor': 'mse',\n",
        "          'discriminator': 'binary_crossentropy'\n",
        "        } \n",
        "\n",
        "        log_dirs = {'logs': './logs', 'weights': './weights'}\n",
        "\n",
        "        fallback_save_every_n_epochs=10\n",
        "\n",
        "        learning_rate = lr\n",
        "\n",
        "        flatness = {'min': 0.0, 'max': 0.15, 'increase': 0.01, 'increase_frequency': 5}\n",
        "\n",
        "        trainer = Trainer(\n",
        "            generator=rrdn,\n",
        "            discriminator=discr,\n",
        "            feature_extractor=f_ext,\n",
        "            lr_train_dir='PATH TO LR TRAINING DATA',\n",
        "            hr_train_dir='PATH TO HR TRAINING DATA',\n",
        "            lr_valid_dir='PATH TO LR VALIDATION DATA',\n",
        "            hr_valid_dir='PATH TO HR VALIDATION DATA',\n",
        "            loss_weights=loss_weights,\n",
        "            learning_rate=learning_rate,\n",
        "            flatness=flatness,\n",
        "            dataname='div2k',\n",
        "            log_dirs=log_dirs,\n",
        "            weights_generator=None,\n",
        "            weights_discriminator=None,\n",
        "            n_validation = 40\n",
        "            )\n",
        "        \n",
        "        trainer.train(\n",
        "        epochs=200,\n",
        "        steps_per_epoch=int(20*4/batch),\n",
        "        batch_size=batch,\n",
        "        monitored_metrics={'val_generator_PSNR_Y':'max'})\n",
        "\n",
        "      except Exception as e:\n",
        "        print('!!!!!!!!!!!!!!!TRAINING FAILED!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
        "        print('batch_size: '+str(batch))\n",
        "        print('patch_size: '+str(patch))\n",
        "        print(lr)\n",
        "        print(e)\n",
        "        time.sleep(90)\n"
      ],
      "metadata": {
        "id": "HYFnXa_B95lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train in loop to test different weight combinations (for RRDN)"
      ],
      "metadata": {
        "id": "R_9fPz1VrngY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_weights_list = [{'generator': 1,'feature_extractor' : 0,'discriminator': 0},\n",
        "                     {'generator': 1,'feature_extractor' : 0.0001,'discriminator': 0.0001},\n",
        "                     {'generator': 1,'feature_extractor' : 0.001,'discriminator': 0.001},\n",
        "                     {'generator': 1,'feature_extractor' : 0.01,'discriminator': 0.01},\n",
        "                     {'generator': 1,'feature_extractor' : 0.04,'discriminator': 0.02}]"
      ],
      "metadata": {
        "id": "JN5IX4QUrt8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "for loss in loss_weights_list:\n",
        "  try:\n",
        "    lr_train_patch_size = 40\n",
        "    layers_to_extract = [5, 9]\n",
        "    scale = 3\n",
        "    hr_train_patch_size = lr_train_patch_size * scale\n",
        "\n",
        "    rrdn  = RRDN(arch_params={'C':4, 'D':3, 'G':64, 'G0':64, 'T':10, 'x':scale}, patch_size=lr_train_patch_size)\n",
        "    f_ext = Cut_VGG19(patch_size=hr_train_patch_size, layers_to_extract=layers_to_extract)\n",
        "    discr = Discriminator(patch_size=hr_train_patch_size, kernel_size=3)\n",
        "\n",
        "    from ISR.train import Trainer\n",
        "    loss_weights = loss\n",
        "    losses = {\n",
        "      'generator': 'mae',\n",
        "      'feature_extractor': 'mse',\n",
        "      'discriminator': 'binary_crossentropy'\n",
        "    } \n",
        "\n",
        "    log_dirs = {'logs': './logs', 'weights': './weights'}\n",
        "\n",
        "    fallback_save_every_n_epochs=10\n",
        "\n",
        "    learning_rate = {'initial_value': 0.0004, 'decay_factor': .5, 'decay_frequency': 30}\n",
        "\n",
        "    flatness = {'min': 0.0, 'max': 0.15, 'increase': 0.01, 'increase_frequency': 5}\n",
        "\n",
        "    trainer = Trainer(\n",
        "        generator=rrdn,\n",
        "        discriminator=discr,\n",
        "        feature_extractor=f_ext,\n",
        "        lr_train_dir='PATH TO LR TRAINING DATA',\n",
        "        hr_train_dir='PATH TO HR TRAINING DATA',\n",
        "        lr_valid_dir='PATH TO LR VALIDATION DATA',\n",
        "        hr_valid_dir='PATH TO HR VALIDATION DATA',\n",
        "        loss_weights=loss_weights,\n",
        "        learning_rate=learning_rate,\n",
        "        flatness=flatness,\n",
        "        dataname='l2_s8',\n",
        "        log_dirs=log_dirs,\n",
        "        weights_generator=None,\n",
        "        weights_discriminator=None,\n",
        "        n_validation = 40\n",
        "        )\n",
        "\n",
        "    trainer.train(\n",
        "    epochs=400,\n",
        "    steps_per_epoch=40,\n",
        "    batch_size=4,\n",
        "    monitored_metrics={'val_generator_PSNR_Y':'max'})\n",
        "\n",
        "  except Exception as e:\n",
        "    print('!!!!!!!!!!!!!!!TRAINING FAILED!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
        "    print(loss)\n",
        "    print(e)\n",
        "    time.sleep(90)"
      ],
      "metadata": {
        "id": "tyxGWmWlruHu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ISR_training_RRDN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}